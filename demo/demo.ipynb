{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086a5808",
   "metadata": {},
   "source": [
    "# Inverse Drum Machine: Separation and Synthesis Demo\n",
    "\n",
    "This notebook demonstrates how to use the models from the Inverse Drum Machine project to perform drum separation. It allows you to:\n",
    "\n",
    "1.  **Load** different pre-trained models (including our method and baselines).\n",
    "2.  **Process** tracks from the StemGMD dataset.\n",
    "3.  **Apply** either direct synthesis or source separation via masking.\n",
    "4.  **Save** the separated stems as audio files for evaluation and listening.\n",
    "\n",
    "This code was used to generate the demos from the demo page at [https://bernardo-torres.github.io/projects/inverse-drum-machine/](https://bernardo-torres.github.io/projects/inverse-drum-machine/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be88db",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, please ensure you have:\n",
    "\n",
    "1.  **Installed Dependencies**: Make sure all required packages are installed.\n",
    "2.  **Downloaded the Dataset**: This demo uses the test tracks from the **StemGMD** dataset. Please download it and place it in the `data/StemGMD` directory.\n",
    "3.  **Downloaded Pre-trained Models**: To use our trained model, download the checkpoint weights and place them in the appropriate `logs/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e6a8f9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ba3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import copy\n",
    "\n",
    "from idm.baselines.larsnet.larsnet import LarsNet\n",
    "from idm.utils import get_normalizing_function\n",
    "from idm.synthesis_conditioning.peak_picking import PeakPicking\n",
    "from idm.utils import cpu_numpy\n",
    "from idm.data.dataset import with_audio_settings\n",
    "from idm.data.datamodule import get_dataset\n",
    "from idm import eval_class_mapping, drum_kit_map_stemgmd\n",
    "from idm.feature_extractor.stft import STFT\n",
    "from idm.inference import estimate_masks, load_model, ROOT_PATH\n",
    "\n",
    "inverse_kit_map = {v: k for k, v in drum_kit_map_stemgmd.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ced39c",
   "metadata": {},
   "source": [
    "### Quick Test\n",
    "With the following code we should be able to load a model and the dataset, and run inference on a single track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eac69e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: idm-44-train-kits\n",
      "Found checkpoint at: logs/idm-44-train-kits/checkpoints/val-epoch=518-global_step=0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardo/anaconda3/envs/ddsp/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n",
      "torch.Size([176400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'stems': tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 7.1568e-11,  2.0448e-11,  1.4825e-10,  ..., -4.4564e-04,\n",
       "            9.4507e-05,  8.0401e-04],\n",
       "          [ 2.8438e-10, -1.2797e-10, -1.7063e-10,  ..., -1.8026e-04,\n",
       "           -3.0684e-04, -3.7834e-04],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       " 'output': tensor([[-1.0469e-08, -2.8224e-09, -1.4152e-08,  ..., -9.5351e-03,\n",
       "          -7.5431e-03, -7.8883e-03]], device='cuda:0', grad_fn=<SumBackward1>),\n",
       " 'samples': tensor([[[-1.1278e-02, -1.1294e-02, -5.3387e-03,  ...,  1.0655e-02,\n",
       "            1.5195e-02,  1.6045e-02],\n",
       "          [-2.3587e-02, -2.3654e-02,  3.5778e-03,  ..., -1.2749e-03,\n",
       "            1.2716e-04,  8.0279e-04],\n",
       "          [-1.6935e-01, -1.6892e-01,  2.8606e-01,  ...,  2.7941e-04,\n",
       "            1.1056e-03,  5.0647e-04],\n",
       "          ...,\n",
       "          [ 8.6537e-02,  8.6757e-02,  8.6299e-02,  ...,  1.1767e-02,\n",
       "            1.1668e-02,  1.1730e-02],\n",
       "          [ 7.5369e-02,  7.5359e-02,  8.2085e-02,  ...,  6.2953e-04,\n",
       "            6.8562e-04,  7.1758e-04],\n",
       "          [ 6.5339e-02,  6.5279e-02,  6.4979e-02,  ...,  2.0196e-03,\n",
       "            2.0468e-03,  2.0698e-03]]], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>),\n",
       " 'onsets': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# MODEL_IDENTIFIER = 'gt'\n",
    "# MODEL_IDENTIFIER = 'oracle'\n",
    "# MODEL_IDENTIFIER = 'larsnet_stereo'\n",
    "# MODEL_IDENTIFIER = 'larsnet_mono'\n",
    "# MODEL_IDENTIFIER = 'nmfd_case1a'\n",
    "MODEL_IDENTIFIER = 'idm-44-train-kits' \n",
    "# DATASET_SPLIT = 'test_test_kits'\n",
    "# DATASET_SPLIT = 'test_train_kits'\n",
    "DATASET_SPLIT = 'eval_session_train_kits'\n",
    "\n",
    "\n",
    "OUTPUT_DIR = Path(\"demo/separation_outputs\")\n",
    "EVAL_SR = 44100\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model, name = load_model(MODEL_IDENTIFIER, DEVICE)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "model_sr = model.sampling_rate if hasattr(model, \"sampling_rate\") else EVAL_SR\n",
    "\n",
    "print(f\"Loading dataset split: {DATASET_SPLIT}\")\n",
    "dataset = get_dataset(\n",
    "    sample_rate_target=EVAL_SR,\n",
    "    root_path=ROOT_PATH,\n",
    "    dataset_root=ROOT_PATH / \"data\" / \"StemGMD\",\n",
    "    dataset_split=DATASET_SPLIT,\n",
    "    version=\"full\",\n",
    "    normalize=False, # Normalize per-track later\n",
    ")\n",
    "class_names = dataset.all_possible_classes\n",
    "print(f\"Dataset has {len(dataset)} tracks and {len(class_names)} classes: {class_names}\")\n",
    "print(dataset[0][\"mix\"][:4*model_sr].shape)\n",
    "model(dataset[0][\"mix\"][:4*model_sr].to(DEVICE)[None, None, :])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb76fc51",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "The `run_all` function encapsulates the entire inference pipeline for multiple models. It iterates through selected tracks in the dataset, performs separation or synthesis, and saves the output audio to disk.\n",
    "Control over which tracks to save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853f7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalizing_fn = get_normalizing_function(\"maxabs\")\n",
    "class_mapping = eval_class_mapping[\"5_class\"]\n",
    "\n",
    "synth_or_masked = \"masked\"  # Choose between \"synth\" or \"masked\"\n",
    "alpha = 1.0  # Masking exponent\n",
    "\n",
    "def run_all(DATASET_SPLIT, MODEL_IDENTIFIER, synth_or_masked=\"masked\", track_indices=None):\n",
    "    model, name = load_model(MODEL_IDENTIFIER, DEVICE)\n",
    "    model_sr = model.sampling_rate if hasattr(model, \"sampling_rate\") else EVAL_SR\n",
    "\n",
    "    print(f\"Loading dataset split: {DATASET_SPLIT}\")\n",
    "    dataset = get_dataset(\n",
    "        sample_rate_target=EVAL_SR,\n",
    "        dataset_split=DATASET_SPLIT,\n",
    "        version=\"full\",\n",
    "        root_path=ROOT_PATH,\n",
    "        dataset_root=ROOT_PATH / \"data\" / \"StemGMD\",\n",
    "        gt_sources_path=ROOT_PATH / \"data\" / \"Stem_GMD_single_hits\",\n",
    "        normalize=False, # Normalize per-track later\n",
    "    )\n",
    "    class_names = dataset.all_possible_classes\n",
    "    print(f\"Dataset has {len(dataset)} tracks and {len(class_names)} classes: {class_names}\")\n",
    "    if track_indices is None:\n",
    "        # Some default values for each split\n",
    "        if DATASET_SPLIT == \"test_test_kits\":\n",
    "            TRACK_INDICES = [108, 219, 240, 256, 265, 268]\n",
    "        elif DATASET_SPLIT == \"test_train_kits\":\n",
    "            TRACK_INDICES = [163, 164, 316, 373, 389, 393]\n",
    "        else:\n",
    "            TRACK_INDICES = [108, 219, 240, 256, 265, 268]\n",
    "    else:\n",
    "        TRACK_INDICES = track_indices\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for track_idx in tqdm(TRACK_INDICES):\n",
    "            batch = dataset[track_idx]\n",
    "            unique_classes = batch[\"all_possible_classes\"]\n",
    "            unique_eval_classes = np.unique(list(class_mapping.values()))\n",
    "\n",
    "            stems = {k: v.to(DEVICE) for k, v in batch[\"stems\"].items()}\n",
    "            eval_mix = sum(stems.values())\n",
    "            input_mix = eval_mix\n",
    "\n",
    "            with with_audio_settings(\n",
    "            dataset, sample_rate_target=EVAL_SR, mono=model.mono if hasattr(model, \"mono\") else True\n",
    "        ):\n",
    "                _batch = dataset[track_idx]\n",
    "                _stems = {k: v.to(DEVICE) for k, v in _batch[\"stems\"].items()}\n",
    "                    # Create a mix from the stems\n",
    "                input_mix = sum(_stems.values())\n",
    "                # Delete _batch and _stems\n",
    "                del _batch, _stems\n",
    "\n",
    "            # Normalize the ground truth mix and stems for a fair comparison\n",
    "            norm_factor = torch.max(torch.abs(eval_mix)) / torch.max(torch.abs(normalizing_fn(eval_mix)))\n",
    "            if norm_factor == 0: norm_factor = 1\n",
    "            eval_mix_norm = eval_mix / norm_factor\n",
    "            stems = {k: v / norm_factor for k, v in stems.items()}\n",
    "            eval_mix = normalizing_fn(eval_mix)\n",
    "            input_mix = normalizing_fn(input_mix)\n",
    "\n",
    "            # Prepare model input (resample if necessary)\n",
    "            if model_sr != EVAL_SR:\n",
    "                input_mix = librosa.resample(cpu_numpy(input_mix), orig_sr=EVAL_SR, target_sr=model_sr)\n",
    "                input_mix = torch.from_numpy(input_mix).to(DEVICE)\n",
    "\n",
    "\n",
    "            # --- Run model ---\n",
    "            if isinstance(model, str) and \"gt\" in MODEL_IDENTIFIER:\n",
    "                tf = False\n",
    "                map_model_outs = True\n",
    "                train_classes = list(dataset.all_possible_classes)\n",
    "                synth_stems = torch.stack([stems[clas] for clas in dataset.all_possible_classes], dim=0).unsqueeze(0)\n",
    "                synth_or_masked = \"synth\"\n",
    "            elif isinstance(model, str) and \"oracle\" in MODEL_IDENTIFIER:\n",
    "                tf = False\n",
    "                map_model_outs = True\n",
    "                train_classes = list(dataset.all_possible_classes)\n",
    "                synth_stems = torch.stack([stems[clas] for clas in dataset.all_possible_classes], dim=0).unsqueeze(0) \n",
    "                synth_or_masked = \"masked\"\n",
    "            elif isinstance(model, LarsNet):\n",
    "                tf = False\n",
    "                map_model_outs = False\n",
    "                train_classes = model.train_classes\n",
    "                if input_mix.ndim == 1:\n",
    "                    assert model.mono is True, \"Model is stereo but input is mono\"\n",
    "                    # Let's duplicate the mono input to stereo\n",
    "                    input_mix = input_mix.unsqueeze(0).repeat(2, 1)\n",
    "                est_stems_dict = model(input_mix.unsqueeze(0))\n",
    "                synth_stems = torch.stack([est_stems_dict[cls].mean(dim=0) for cls in unique_eval_classes], dim=0).unsqueeze(0)\n",
    "            elif hasattr(model, \"encoder\"): # Assumes our trained model structure\n",
    "                tf = False\n",
    "                map_model_outs = True\n",
    "                train_classes = model.train_classes\n",
    "                encoder_outs = model.encoder(input_mix.unsqueeze(0))\n",
    "                peak_picking_fn = PeakPicking(activation_rate=encoder_outs['activation_rate'], classes=model.train_classes)\n",
    "                model.decoder.peak_picking_fn = peak_picking_fn\n",
    "                outputs = model.decoder(**encoder_outs, extra_returns=[\"stems\"])\n",
    "                synth_stems = outputs[\"stems\"]\n",
    "            else:\n",
    "                tf = True\n",
    "                synth_or_masked=\"masked\"\n",
    "                map_model_outs = True\n",
    "                train_classes = model.train_classes\n",
    "                sources = batch[\"gt_sources\"]\n",
    "                onsets = batch[\"onsets_dict\"]\n",
    "                # Convert onsets to torch tensors\n",
    "                for key in onsets:\n",
    "                    onsets[key] = torch.tensor(onsets[key], device=DEVICE)\n",
    "                sources = torch.stack([sources[inst] for inst in unique_classes], dim=0)\n",
    "                output = model(\n",
    "                    input_mix.unsqueeze(0),\n",
    "                    sources=sources,\n",
    "                    ext_onsets=onsets,\n",
    "                    refit=True,\n",
    "                    return_keys=[\"spec_output\"],\n",
    "                )\n",
    "                synth_stems = output[\"spec_output\"]\n",
    "\n",
    "\n",
    "            # Resample output back to eval sample rate if necessary\n",
    "            # if model_sr != EVAL_SR:\n",
    "            #     est_stems = soxr_resample_batched(est_stems.unsqueeze(0), model_sr, EVAL_SR).squeeze(0)\n",
    "\n",
    "            if not tf:\n",
    "                synth_stems = synth_stems[..., : eval_mix.shape[-1]] \n",
    "\n",
    "            if map_model_outs:\n",
    "                mapped_synth_stems = torch.zeros(\n",
    "                    (synth_stems.shape[0], len(unique_eval_classes), *synth_stems.shape[2:]), device=synth_stems.device\n",
    "                )\n",
    "            else:\n",
    "                mapped_synth_stems = synth_stems\n",
    "\n",
    "            mapped_gt_stems = torch.zeros((len(unique_eval_classes), eval_mix.shape[-1]), device=eval_mix.device)\n",
    "            for class_name in unique_classes:\n",
    "                mapped_class = class_mapping[class_name]\n",
    "\n",
    "                # If any of the stems in the class is active, the class is active\n",
    "                eval_clas_idx = np.where(unique_eval_classes == mapped_class)[0][0]\n",
    "                mapped_gt_stems[eval_clas_idx] += stems[class_name]\n",
    "                if class_name not in train_classes:\n",
    "                    continue\n",
    "                idx = train_classes.index(class_name)\n",
    "                if map_model_outs:  # If we have more train classes than eval classes\n",
    "                    mapped_synth_stems[:, eval_clas_idx] += synth_stems[:, idx]\n",
    "\n",
    "            synth_stems = mapped_synth_stems\n",
    "            stems = mapped_gt_stems\n",
    "\n",
    "            # Trim to match original length\n",
    "            if synth_or_masked == \"masked\":\n",
    "                transform = STFT(n_fft=1024, hop_length=256, win_length=1024, center=True, magnitude=False) if not tf else model.transform\n",
    "\n",
    "                if tf: # If the model already outputs TF domain (e.g., NMFD)\n",
    "                    synth_tf = synth_stems\n",
    "                    transform = copy.deepcopy(model.transform)\n",
    "                    transform.magnitude = False\n",
    "                else:\n",
    "                    synth_tf = transform(synth_stems)\n",
    "                mix_tf = transform(eval_mix)\n",
    "                masked_stems = estimate_masks(mix_tf, synth_tf, masking_type=\"wiener\", alpha=alpha)\n",
    "                synth_stems = transform.inverse(masked_stems, length=eval_mix.shape[-1])\n",
    "            \n",
    "            # --- Save to disk ---\n",
    "            drum_kit = batch['drum_kit']\n",
    "            # f\"{batch['audio_fn']}\".split('.')[0]_f\"{inverse_kit_map[drum_kit]}\"\n",
    "            FILE_KIT = f\"{batch['audio_fn']}\".split('.')[0] + f\"_{inverse_kit_map[drum_kit]}\"\n",
    "            save_dir = OUTPUT_DIR / DATASET_SPLIT / FILE_KIT / MODEL_IDENTIFIER \n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Let's also save the mix\n",
    "            sf.write(save_dir.parent / f\"mix.wav\", eval_mix.cpu().numpy(), EVAL_SR)\n",
    "            # Now let's save every mix and stem\n",
    "            for i, cls in enumerate(unique_eval_classes):\n",
    "                est_stem = synth_stems[:, i].squeeze(0).cpu().numpy()\n",
    "\n",
    "                sf.write(save_dir / f\"{cls}_{synth_or_masked}.wav\", est_stem, EVAL_SR)\n",
    "    print(\"\\n--- Inference Complete ---\")\n",
    "    print(f\"Saved separated tracks to: {os.path.relpath(OUTPUT_DIR.resolve(), ROOT_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5e5aec",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Now, we execute the `run_all` function to generate the separated tracks.\n",
    "\n",
    "The following cell will run inference for multiple models and configurations. You can comment out any run you don't want to execute. The results will be saved in the `demo/separation_outputs` directory. \n",
    "\n",
    "If you decide to manually select the tracks, be sure to inspect the dataset to check how many tracks are available in each split. They are sorted by duration, so the last indices correspond to very long tracks (30s+).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1937ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: idm-44-train-kits\n",
      "Found checkpoint at: logs/idm-44-train-kits/checkpoints/val-epoch=518-global_step=0.ckpt\n",
      "Loading dataset split: eval_session_train_kits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bernardo/anaconda3/envs/ddsp/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: oracle\n",
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:10<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: larsnet\n",
      "Loading UNet models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CY pretrained_cymbals_unet: 100%|██████████| 5/5 [00:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: larsnet-mono\n",
      "Loading UNet models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CY pretrained_cymbals_unet: 100%|██████████| 5/5 [00:00<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: nmfd_case1a\n",
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:08<00:07,  2.59s/it][rank: 0] Clamping onset frame 3122 to 3104. This may indicate a mismatch between activation_rate and n_frames.\n",
      " 67%|██████▋   | 4/6 [00:10<00:05,  2.56s/it][rank: 0] Clamping onset frame 4100 to 4097. This may indicate a mismatch between activation_rate and n_frames.\n",
      "100%|██████████| 6/6 [00:16<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: gt\n",
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:09<00:00,  1.65s/it]\n",
      "/home/bernardo/anaconda3/envs/ddsp/lib/python3.10/site-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: idm-44-train-kits\n",
      "Found checkpoint at: logs/idm-44-train-kits/checkpoints/val-epoch=518-global_step=0.ckpt\n",
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:11<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: larsnet\n",
      "Loading UNet models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CY pretrained_cymbals_unet: 100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n",
      "Loading model: larsnet-mono\n",
      "Loading UNet models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CY pretrained_cymbals_unet: 100%|██████████| 5/5 [00:00<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset split: eval_session_train_kits\n",
      "Dataset has 240 tracks and 9 classes: ['CY_CR' 'CY_RD' 'HH_CHH' 'HH_OHH' 'KD' 'SD' 'TT_HFT' 'TT_HMT' 'TT_LMT']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Complete ---\n",
      "Saved separated tracks to: demo/separation_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATASET_SPLIT = \"eval_session_train_kits\"\n",
    "\n",
    "# Masked-based runs\n",
    "run_all(DATASET_SPLIT, \"idm-44-train-kits\", synth_or_masked=\"masked\")\n",
    "run_all(DATASET_SPLIT, \"oracle\", synth_or_masked=\"masked\")\n",
    "run_all(DATASET_SPLIT, \"larsnet\", synth_or_masked=\"masked\")\n",
    "run_all(DATASET_SPLIT, \"larsnet-mono\", synth_or_masked=\"masked\")\n",
    "run_all(DATASET_SPLIT, \"nmfd_case1a\", synth_or_masked=\"masked\")\n",
    "\n",
    "# Direct synthesis runs\n",
    "run_all(DATASET_SPLIT, \"gt\", synth_or_masked=\"synth\")\n",
    "run_all(DATASET_SPLIT, \"idm-44-train-kits\", synth_or_masked=\"synth\")\n",
    "run_all(DATASET_SPLIT, \"larsnet\", synth_or_masked=\"synth\")\n",
    "run_all(DATASET_SPLIT, \"larsnet-mono\", synth_or_masked=\"synth\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
